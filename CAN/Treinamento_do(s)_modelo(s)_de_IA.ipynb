{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from joblib import dump#, load\n",
    "\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import MinMaxScaler#, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model, Sequential#, load_model\n",
    "from keras.layers import Input, LSTM, RepeatVector, Dense, Dropout, BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo o tema do seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição da SEED:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED) # Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregando os dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os valores são números muito pequenos com muitas casas decimais, por isso é bom que o dataframe consiga representar isso também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', '{:.20f}'.format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando os dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign = pd.read_csv(\"data/dados benignos/mensagens_benignas.csv\")\n",
    "df_benign_2 = pd.read_csv(\"data/dados benignos/mensagens_benignas_2.csv\")\n",
    "df_benign = pd.concat([df_benign, df_benign_2], axis=0)\n",
    "\n",
    "del df_benign_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious_random_dos = pd.read_csv(\"data/ataques/mensagens_maliciosas_random_dos.csv\")\n",
    "df_malicious_spoofing_zero_payload = pd.read_csv(\"data/ataques/mensagens_maliciosas_spoofing_zero_payload.csv\")\n",
    "df_malicious_zero_dos = pd.read_csv(\"data/ataques/mensagens_maliciosas_zero_dos.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious_random_dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious_spoofing_zero_payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_malicious_zero_dos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manipualação da quantidade/proporção dos dados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPCIONAL!!!\n",
    "len_min = min(len(df_benign),len(df_malicious_random_dos),len(df_malicious_spoofing_zero_payload),len(df_malicious_zero_dos))\n",
    "NUM_OF_ATTACKS = 3\n",
    "\n",
    "#len_min = int(len_min / 40)\n",
    "\n",
    "#df_benign = df_benign.head(len_min * NUM_OF_ATTACKS)\n",
    "df_malicious_random_dos = df_malicious_random_dos.head(len_min)\n",
    "df_malicious_spoofing_zero_payload = df_malicious_spoofing_zero_payload.head(len_min)\n",
    "df_malicious_zero_dos = df_malicious_zero_dos.head(len_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del len_min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratando dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalização dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação e uso do scaler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaler.fit(df_benign)\n",
    "\n",
    "df_benign_scaled = pd.DataFrame(scaler.transform(df_benign), columns=df_benign.columns, index=df_benign.index)\n",
    "df_malicious_random_dos_scaled = pd.DataFrame(scaler.transform(df_malicious_random_dos), columns=df_malicious_random_dos.columns, index=df_malicious_random_dos.index)\n",
    "df_malicious_spoofing_zero_payload_scaled = pd.DataFrame(scaler.transform(df_malicious_spoofing_zero_payload), columns=df_malicious_spoofing_zero_payload.columns, index=df_malicious_spoofing_zero_payload.index)\n",
    "df_malicious_zero_dos_scaled = pd.DataFrame(scaler.transform(df_malicious_zero_dos), columns=df_malicious_zero_dos.columns, index=df_malicious_zero_dos.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_benign\n",
    "del df_malicious_random_dos\n",
    "del df_malicious_spoofing_zero_payload\n",
    "del df_malicious_zero_dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_benign_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_labels_benign = [1] * len(df_benign_scaled)\n",
    "list_labels_random_dos = [-1] * len(df_malicious_random_dos_scaled)\n",
    "list_labels_spoofing_zero_payload = [-1] * len(df_malicious_spoofing_zero_payload_scaled)\n",
    "list_labels_zero_dos = [-1] * len(df_malicious_zero_dos_scaled)\n",
    "\n",
    "classifier_type = \"bc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criação de Janelas Temporais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação de Janelas Deslizantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação da função de divisão dos dados em janelas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slicing_windows(data, labels, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - time_step):\n",
    "        a = data[i:(i + time_step)]\n",
    "        X.append(a)\n",
    "        Y.append(labels[i + time_step])\n",
    "    return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição do tamanho da janela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criação das janelas deslizantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_windows, benign_labels = create_slicing_windows(df_benign_scaled, list_labels_benign, WINDOW_SIZE)\n",
    "del df_benign_scaled, list_labels_benign\n",
    "\n",
    "malicious_random_dos_windows, malicious_random_dos_labels = create_slicing_windows(df_malicious_random_dos_scaled, list_labels_random_dos, WINDOW_SIZE)\n",
    "del df_malicious_random_dos_scaled, list_labels_random_dos\n",
    "\n",
    "malicious_spoofing_zero_payload_windows, malicious_spoofing_zero_payload_labels = create_slicing_windows(df_malicious_spoofing_zero_payload_scaled, list_labels_spoofing_zero_payload, WINDOW_SIZE)\n",
    "del df_malicious_spoofing_zero_payload_scaled, list_labels_spoofing_zero_payload\n",
    "\n",
    "malicious_zero_dos_windows, malicious_zero_dos_labels = create_slicing_windows(df_malicious_zero_dos_scaled, list_labels_zero_dos, WINDOW_SIZE)\n",
    "del df_malicious_zero_dos_scaled, list_labels_zero_dos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(benign_windows), len(benign_windows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_windows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividindo dados em Treino, Validação e Teste"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade de dados benignos dividido pelo total:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(benign_windows) / (len(benign_windows) + len(malicious_random_dos_windows) + len(malicious_spoofing_zero_payload_windows) + len(malicious_zero_dos_windows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenação das janelas, em ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.vstack((benign_windows, malicious_random_dos_windows, malicious_spoofing_zero_payload_windows, malicious_zero_dos_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_malicious = np.vstack((malicious_random_dos_windows, malicious_spoofing_zero_payload_windows, malicious_zero_dos_windows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = np.hstack((benign_labels, malicious_random_dos_labels, malicious_spoofing_zero_payload_labels, malicious_zero_dos_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_malicious_labels = np.hstack((malicious_random_dos_labels, malicious_spoofing_zero_payload_labels, malicious_zero_dos_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del malicious_random_dos_windows\n",
    "del malicious_spoofing_zero_payload_windows\n",
    "del malicious_zero_dos_windows\n",
    "del malicious_random_dos_labels\n",
    "del malicious_spoofing_zero_payload_labels\n",
    "del malicious_zero_dos_labels\n",
    "del benign_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão em treino, validação e teste."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abordagem supervisionada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_test_data, train_labels, val_test_labels = train_test_split(data, data_labels, test_size=0.25, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_data, test_data, val_labels, test_labels = train_test_split(val_test_data, val_test_labels, test_size=0.5, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, data_labels\n",
    "del val_test_data, val_test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Abordagem não-supervisionada:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2 = benign_windows[:int((len(benign_windows) // 1.05))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_2 = np.vstack((data_malicious, benign_windows[int((len(benign_windows) // 1.05)):]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "malicious_proportion = len(data_malicious) / (len(data_malicious) + len(benign_windows[int((len(benign_windows) // 1.05)):]))\n",
    "malicious_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benign_labels = [1] * int(len(benign_windows) - (len(benign_windows) // 1.05))\n",
    "\n",
    "test_data_2_labels = np.hstack((data_malicious_labels, benign_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_malicious\n",
    "del benign_windows\n",
    "del benign_labels\n",
    "del data_malicious_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape dos dados de treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IAs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (supervisionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variável que define se é supervisionado ou não-supervisionado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_OR_ns = \"s\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantidade de features, para o modelo lidar com as entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COUNT = train_data.shape[2] # Número de features dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção do modelo LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o modelo LSTM\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, activation='tanh', input_shape=(WINDOW_SIZE, FEATURES_COUNT)))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1, kernel_regularizer=l2(0.0000001)))\n",
    "\n",
    "# Compilando o modelo\n",
    "model.compile(optimizer=Adam(learning_rate=0.004), loss='mse')\n",
    "#model.compile(optimizer=\"adam\", loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição da paciência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração do early stop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definição da quantidade de épocas e o tamanho do Batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32 #padrão: batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TREINAMENTO DO MODELO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinando o modelo\n",
    "history = model.fit(train_data, train_labels, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_data=(val_data, val_labels), callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data, train_labels\n",
    "del val_data, val_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avaliando o modelo no conjunto de teste\n",
    "loss = model.evaluate(test_data, test_labels)\n",
    "print(\"Test Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test_data)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliações"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando as predições em uma numpy array unidimensional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_1d = np.array([predict[0] for predict in predicts])\n",
    "predicts_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_true, y_score):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(test_labels, predicts_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, max_fpr=1.0):\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "  aucroc = roc_auc_score(y_true, y_score)\n",
    "  plt.plot(100*fpr[fpr < max_fpr], 100*tpr[fpr < max_fpr], label=f'ROC Curve (AUC = {aucroc:.4f})')\n",
    "  plt.xlim(-2,102)\n",
    "  plt.xlabel('FPR (%)')\n",
    "  plt.ylabel('TPR (%)')\n",
    "  plt.legend()\n",
    "  plt.title('ROC Curve and AUCROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(test_labels, predicts_1d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformando todos os valores de predições acima de 0 em 1, enquanto os outros recebem a função round(), ou seja, serão arredondados pro número inteiro mais próximo. Após isso, os números que forem iguais a 0, serão transformados em -1. É como se todo mundo igual ou menor que 0, fosse malicioso, mas 0 não é um valor malicioso de label, é mais como se fosse um threshold, então nada deve ser igual a 0. Esse problema é minimizado em classificações binárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts_1d = np.where(predicts_1d > 0, 1, np.round(predicts_1d))\n",
    "predicts_1d = np.where(predicts_1d == 0, -1, predicts_1d)\n",
    "predicts_1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  group_counts = [f'{value:.0f}' for value in confusion_matrix(y_true, y_pred).ravel()]\n",
    "  group_percentages = [f'{value*100:.2f}%' for value in confusion_matrix(y_true, y_pred).ravel()/np.sum(cm)]\n",
    "  labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
    "  labels = np.array(labels).reshape(2,2)\n",
    "  sns.heatmap(cm, annot=labels, cmap='Oranges', xticklabels=['Predicted Benign', 'Predicted Malicious'], yticklabels=['Actual Benign', 'Actual Malicious'], fmt='')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_labels, predicts_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_metrics(y_true, y_pred):\n",
    "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "  tpr = tp/(tp+fn)\n",
    "  fpr = fp/(fp+tn)\n",
    "  precision = tp/(tp+fp)\n",
    "  f1 = (2*tpr*precision)/(tpr+precision)\n",
    "  return {'acc':acc,'tpr':tpr,'fpr':fpr,'precision':precision,'f1-score':f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas Gerais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_overall_metrics(test_labels, predicts_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_data, test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando modelo com diferentes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(model, arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, f\"models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, f\"models/scalers/scaler_model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del history\n",
    "del loss\n",
    "del predicts\n",
    "del predicts_1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM (não-supervisionado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento do modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo que é não-supervisionado(ns) e classificador binário(bc):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_OR_ns = \"ns\"\n",
    "classifier_type = \"bc\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coletando a quantidade de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_COUNT = train_data_2.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construção do modelo LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construindo o modelo Autoencoder LSTM\n",
    "inputs = Input(shape=(WINDOW_SIZE, FEATURES_COUNT))\n",
    "encoded = LSTM(50, activation='tanh', return_sequences=False)(inputs)  # Codificador\n",
    "decoded = RepeatVector(WINDOW_SIZE)(encoded)  # Decodificador\n",
    "decoded = LSTM(FEATURES_COUNT, return_sequences=True)(decoded)\n",
    "\n",
    "# Adicionando Dropout e Normalização em Batch\n",
    "decoded = Dropout(0.1)(decoded)\n",
    "decoded = BatchNormalization()(decoded)\n",
    "\n",
    "# Camada de saída\n",
    "outputs = Dense(FEATURES_COUNT, kernel_regularizer=l2(0.0000001))(decoded)\n",
    "\n",
    "# Compilando o modelo\n",
    "model = Model(inputs, outputs)\n",
    "model.compile(optimizer=Adam(learning_rate=0.004), loss='mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a paciência:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATIENCE = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configurando o early stopping:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo a quantidade de épocas e o tamanho do Batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 50\n",
    "BATCH_SIZE = 32 #padrão: batch_size=32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TREINAMENTO DO MODELO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_data_2, train_data_2, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predições e Classificações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts = model.predict(test_data_2)\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_square_error(original_data, predicted_data, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Função para classificar as amostras como benignas (1) ou malignas (-1) com base no erro de reconstrução.\n",
    "\n",
    "    Parâmetros:\n",
    "        original_data (numpy.array): Os dados originais.\n",
    "        predicted_data (numpy.array): As previsões geradas pelo modelo autoencoder.\n",
    "        threshold (float): O limiar de decisão para classificar as amostras.\n",
    "\n",
    "    Retorna:\n",
    "        numpy.array: Um array de classificação binária para cada amostra.\n",
    "    \"\"\"\n",
    "    # Calcula o erro de reconstrução (MSE) para cada amostra\n",
    "    reconstruction_errors = np.mean(np.square(original_data - predicted_data), axis=(1, 2))\n",
    "\n",
    "    # Classifica as amostras com base no limiar\n",
    "    classifications = np.where(reconstruction_errors < threshold, 1, -1)\n",
    "\n",
    "    #del reconstruction_errors\n",
    "\n",
    "    return classifications, reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_error(original_data, reconstructed_data, threshold=0.5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calcula o Erro Absoluto Médio (MAE) entre os dados originais e reconstruídos.\n",
    "\n",
    "    Args:\n",
    "    - original_data: array NumPy contendo os dados originais.\n",
    "    - reconstructed_data: array NumPy contendo os dados reconstruídos pelo modelo.\n",
    "\n",
    "    Returns:\n",
    "    - mae: o Erro Absoluto Médio entre os dados originais e reconstruídos.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # Calcula a diferença absoluta entre os dados originais e reconstruídos\n",
    "    # Calcula o MAE como a média da diferença absoluta\n",
    "    mae = np.mean((np.abs(original_data - reconstructed_data)), axis=(1, 2))\n",
    "\n",
    "    classifications = np.where(mae < threshold, 1, -1)\n",
    "\n",
    "    del mae\n",
    "\n",
    "    return classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold_mse(original_data, predicted_data, proportion, threshold=0.5):\n",
    "\n",
    "    reconstruction_errors = np.mean(np.square(original_data - predicted_data), axis=(1, 2))\n",
    "\n",
    "    classifications = np.where(reconstruction_errors < threshold, 1, -1)\n",
    "\n",
    "    del reconstruction_errors\n",
    "\n",
    "    proportion_predicted = len(classifications[classifications == -1]) / len(classifications)\n",
    "\n",
    "    del classifications\n",
    "\n",
    "    print(f\"Threshold: {threshold}, Proportion_predicted: {proportion_predicted}\")\n",
    "\n",
    "    if proportion_predicted < proportion:\n",
    "        del proportion_predicted\n",
    "        return find_best_threshold_mse(original_data, predicted_data, proportion, 0.9*threshold)\n",
    "    elif proportion_predicted > proportion:\n",
    "        del proportion_predicted\n",
    "        return find_best_threshold_mse(original_data, predicted_data, proportion, 1.1*threshold)\n",
    "    else:\n",
    "        del proportion_predicted\n",
    "        return threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Encontra o melhor threshold baseado na proporção correta entre dados malignos e benignos.\n",
    "\n",
    "    Parameters:\n",
    "    y_true (np.array): Array de valores verdadeiros (-1 para benigno, 1 para maligno)\n",
    "    y_scores (np.array): Array de scores preditos pelo modelo\n",
    "\n",
    "    Returns:\n",
    "    float: Melhor threshold encontrado\n",
    "    \"\"\"\n",
    "    \n",
    "    # Certificar que os inputs são arrays numpy\n",
    "    y_true = np.array(y_true)\n",
    "    y_scores = np.array(y_scores)\n",
    "    \n",
    "    # Ordenar os scores e calcular possíveis thresholds\n",
    "    thresholds = np.sort(y_scores)\n",
    "    \n",
    "    # Inicializar variáveis para acompanhar o melhor threshold e melhor acurácia ponderada\n",
    "    best_threshold = None\n",
    "    best_weighted_accuracy = -np.inf\n",
    "    \n",
    "    # Calcular proporção dos dados malignos e benignos\n",
    "    prop_malignos = np.sum(y_true == 1) / len(y_true)\n",
    "    prop_benignos = np.sum(y_true == -1) / len(y_true)\n",
    "    \n",
    "    for threshold in tqdm(thresholds):\n",
    "        # Calcular predições baseadas no threshold atual\n",
    "        y_pred = np.where(y_scores >= threshold, 1, -1)\n",
    "        \n",
    "        # Calcular acurácia ponderada\n",
    "        accuracy_malignos = np.sum((y_true == 1) & (y_pred == 1)) / np.sum(y_true == 1)\n",
    "        accuracy_benignos = np.sum((y_true == -1) & (y_pred == -1)) / np.sum(y_true == -1)\n",
    "        \n",
    "        weighted_accuracy = prop_malignos * accuracy_malignos + prop_benignos * accuracy_benignos\n",
    "        \n",
    "        # Atualizar melhor threshold se a acurácia ponderada atual for melhor\n",
    "        if weighted_accuracy > best_weighted_accuracy:\n",
    "            best_weighted_accuracy = weighted_accuracy\n",
    "            best_threshold = threshold\n",
    "    \n",
    "    return best_threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Registrando os erros de reconstrução:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_mse, reconstruction_errors = mean_square_error(test_data_2, predicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encontrando um bom Threshold:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = find_best_threshold(test_data_2_labels, reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classificando de acordo com o Threshold encontrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_mse, reconstruction_errors = mean_square_error(test_data_2, predicts, THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Avaliações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(classifications_mse[classifications_mse == -1]) / len(classifications_mse), malicious_proportion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconstruction_errors_rounded = np.where(reconstruction_errors > THRESHOLD, -1, 1)\n",
    "reconstruction_errors_rounded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_score, max_fpr=1.0):\n",
    "  fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
    "  aucroc = roc_auc_score(y_true, y_score)\n",
    "  plt.plot(100*fpr[fpr < max_fpr], 100*tpr[fpr < max_fpr], label=f'ROC Curve (AUC = {aucroc:.4f})')\n",
    "  plt.xlim(-2,102)\n",
    "  plt.xlabel('FPR (%)')\n",
    "  plt.ylabel('TPR (%)')\n",
    "  plt.legend()\n",
    "  plt.title('ROC Curve and AUCROC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva ROC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(test_data_2_labels, 1/reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(y_true, y_score):\n",
    "    precision, recall, _ = precision_recall_curve(y_true, y_score)\n",
    "    plt.plot(recall, precision, marker='.')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Curva Recall:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_precision_recall_curve(test_data_2_labels, 1/reconstruction_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "  cm = confusion_matrix(y_true, y_pred)\n",
    "  group_counts = [f'{value:.0f}' for value in confusion_matrix(y_true, y_pred).ravel()]\n",
    "  group_percentages = [f'{value*100:.2f}%' for value in confusion_matrix(y_true, y_pred).ravel()/np.sum(cm)]\n",
    "  labels = [f'{v1}\\n{v2}' for v1, v2 in zip(group_counts, group_percentages)]\n",
    "  labels = np.array(labels).reshape(2,2)\n",
    "  sns.heatmap(cm, annot=labels, cmap='Oranges', xticklabels=['Predicted Benign', 'Predicted Malicious'], yticklabels=['Actual Benign', 'Actual Malicious'], fmt='')\n",
    "  return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matriz de Confusão:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(test_data_2_labels, reconstruction_errors_rounded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_overall_metrics(y_true, y_pred):\n",
    "  tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "  acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "  tpr = tp/(tp+fn)\n",
    "  fpr = fp/(fp+tn)\n",
    "  precision = tp/(tp+fp)\n",
    "  f1 = (2*tpr*precision)/(tpr+precision)\n",
    "  return {'acc':acc,'tpr':tpr,'fpr':fpr,'precision':precision,'f1-score':f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Métricas Gerais:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_overall_metrics(test_data_2_labels, reconstruction_errors_rounded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando o modelo:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvando modelo com diferentes bibliotecas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = str(THRESHOLD).replace(\".\",\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(f'models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}_t{THRESHOLD}.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}_t{THRESHOLD}.pkl', 'wb') as arquivo:\n",
    "    pickle.dump(model, arquivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(model, f\"models/model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}_t{THRESHOLD}.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(scaler, f\"models/scalers/scaler_model_LSTM_{s_OR_ns}_{classifier_type}_ws{WINDOW_SIZE}_t{THRESHOLD}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
